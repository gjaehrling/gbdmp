{
  "paragraphs": [
    {
      "text": "%md\n#Analyze World Development Indicators using SparkQL\n\nRead data from world-development-indicators-release-2016-01-28-06-31-53.zip\n### Data Structure\n\n- CountryName\n- CountryCode\n- IndiciatorName\n- IndicatorCode\n- Year\n- Value",
      "dateUpdated": "Mar 21, 2016 3:58:19 PM",
      "config": {
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457813589359_201825521",
      "id": "20160312-211309_1237389595",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003eAnalyze World Development Indicators using SparkQL\u003c/h1\u003e\n\u003cp\u003eRead data from world-development-indicators-release-2016-01-28-06-31-53.zip\u003c/p\u003e\n\u003ch3\u003eData Structure\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCountryName\u003c/li\u003e\n\u003cli\u003eCountryCode\u003c/li\u003e\n\u003cli\u003eIndiciatorName\u003c/li\u003e\n\u003cli\u003eIndicatorCode\u003c/li\u003e\n\u003cli\u003eYear\u003c/li\u003e\n\u003cli\u003eValue\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Mar 12, 2016 9:13:09 PM",
      "dateStarted": "Mar 12, 2016 9:31:30 PM",
      "dateFinished": "Mar 12, 2016 9:31:30 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\ncd /Users/hadoop/hadoop-1.2.1\nbin/hadoop fs -mkdir data\nbin/hadoop fs -copyFromLocal /Users/hadoop/Documents/programs/Spark-Masterclass-master/europe-indicators.csv data\nbin/hadoop fs -ls data",
      "dateUpdated": "Mar 21, 2016 3:58:19 PM",
      "config": {
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/sh",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457813681411_-389210086",
      "id": "20160312-211441_1311589974",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "mkdir: cannot create directory data: File exists\nFound 2 items\n-rw-r--r--   1 hadoop supergroup   61626644 2016-03-21 12:13 /user/hadoop/data/europe-indicators.csv\n-rw-r--r--   1 hadoop supergroup       4821 2016-03-21 11:42 /user/hadoop/data/iris.csv\n"
      },
      "dateCreated": "Mar 12, 2016 9:14:41 PM",
      "dateStarted": "Mar 21, 2016 12:13:07 PM",
      "dateFinished": "Mar 21, 2016 12:13:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nimport re\nfrom pyspark.sql.types import *\n\nschema \u003d StructType([ \\\n   StructField(\"CountryName\",   StringType(),  True), \\\n   StructField(\"CountryCode\",   StringType(),  True), \\\n   StructField(\"IndicatorName\", StringType(),  True), \\\n   StructField(\"IndicatorCode\", StringType(),  True), \\\n   StructField(\"Year\",          IntegerType(), True), \\\n   StructField(\"Value\",         FloatType(),  True)  \\\n])\n\ndef split_line(l):\n    tmp \u003d re.sub(r\u0027\\\"(.+),(.+)\\\"\u0027, r\"\\1\\2\", l)\n    return re.sub(r\u0027(\\w+),(\\s+)(.*)\u0027, r\u0027\\1\\2\\3\u0027, tmp)\n    \n    \nraw_data \u003d sc.textFile(\"data/europe-indicators.csv\").map(split_line).map(lambda l: l.split(\",\"))\n\nheader \u003d raw_data.first()\nindicators \u003d raw_data.filter(lambda h: h !\u003d header).filter(lambda l: len(l)\u003e0)\nindicators \u003d indicators.map(lambda row: (str(row[0]), str(row[1]), str(row[2]), str(row[3]).replace(\".\", \"_\"), int(row[4]), float(row[5])))\n\nindicatorsSample \u003d indicators.sample(False, 10, 42)\n\nprint \"\\nraw data format:\\n\"\n\nfor x in raw_data.take(10):\n    print x\n \nprint \"\\nconverted: \\n\"\n\nfor x in indicators.take(10):\n    print x    \n \nindicatorsDF \u003d sqlContext.createDataFrame(indicators, schema) \nsqlContext.registerDataFrameAsTable(indicatorsDF, \"Indicators\")\n",
      "dateUpdated": "Mar 21, 2016 3:58:19 PM",
      "config": {
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457813884203_-1569929484",
      "id": "20160312-211804_1053341868",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nraw data format:\n\n[u\u0027CountryName\u0027, u\u0027CountryCode\u0027, u\u0027IndiciatorName\u0027, u\u0027IndicatorCode\u0027, u\u0027Year\u0027, u\u0027Value\u0027]\n[u\u0027Austria\u0027, u\u0027AUT\u0027, u\u0027Adolescent fertility rate (births per 1000 women ages 15-19)\u0027, u\u0027SP.ADO.TFRT\u0027, u\u00271960\u0027, u\u002748.3914\u0027]\n[u\u0027Austria\u0027, u\u0027AUT\u0027, u\u0027Age dependency ratio (% of working-age population)\u0027, u\u0027SP.POP.DPND\u0027, u\u00271960\u0027, u\u002752.348170383459205\u0027]\n[u\u0027Austria\u0027, u\u0027AUT\u0027, u\u0027Age dependency ratio old (% of working-age population)\u0027, u\u0027SP.POP.DPND.OL\u0027, u\u00271960\u0027, u\u002718.4951086719202\u0027]\n[u\u0027Austria\u0027, u\u0027AUT\u0027, u\u0027Age dependency ratio young (% of working-age population)\u0027, u\u0027SP.POP.DPND.YG\u0027, u\u00271960\u0027, u\u002733.853061711539\u0027]\n[u\u0027Austria\u0027, u\u0027AUT\u0027, u\u0027Alternative and nuclear energy (% of total energy use)\u0027, u\u0027EG.USE.COMM.CL.ZS\u0027, u\u00271960\u0027, u\u00278.96433606078842\u0027]\n[u\u0027Austria\u0027, u\u0027AUT\u0027, u\u0027Arms imports (SIPRI trend indicator values)\u0027, u\u0027MS.MIL.MPRT.KD\u0027, u\u00271960\u0027, u\u00277000000.0\u0027]\n[u\u0027Austria\u0027, u\u0027AUT\u0027, u\u0027Birth rate crude (per 1000 people)\u0027, u\u0027SP.DYN.CBRT.IN\u0027, u\u00271960\u0027, u\u002717.9\u0027]\n[u\u0027Austria\u0027, u\u0027AUT\u0027, u\u0027CO2 emissions (kg per 2005 US$ of GDP)\u0027, u\u0027EN.ATM.CO2E.KD.GD\u0027, u\u00271960\u0027, u\u00270.388626120450145\u0027]\n[u\u0027Austria\u0027, u\u0027AUT\u0027, u\u0027CO2 emissions (kt)\u0027, u\u0027EN.ATM.CO2E.KT\u0027, u\u00271960\u0027, u\u002730821.135\u0027]\n\nconverted: \n\n(\u0027Austria\u0027, \u0027AUT\u0027, \u0027Adolescent fertility rate (births per 1000 women ages 15-19)\u0027, \u0027SP_ADO_TFRT\u0027, 1960, 48.3914)\n(\u0027Austria\u0027, \u0027AUT\u0027, \u0027Age dependency ratio (% of working-age population)\u0027, \u0027SP_POP_DPND\u0027, 1960, 52.348170383459205)\n(\u0027Austria\u0027, \u0027AUT\u0027, \u0027Age dependency ratio old (% of working-age population)\u0027, \u0027SP_POP_DPND_OL\u0027, 1960, 18.4951086719202)\n(\u0027Austria\u0027, \u0027AUT\u0027, \u0027Age dependency ratio young (% of working-age population)\u0027, \u0027SP_POP_DPND_YG\u0027, 1960, 33.853061711539)\n(\u0027Austria\u0027, \u0027AUT\u0027, \u0027Alternative and nuclear energy (% of total energy use)\u0027, \u0027EG_USE_COMM_CL_ZS\u0027, 1960, 8.96433606078842)\n(\u0027Austria\u0027, \u0027AUT\u0027, \u0027Arms imports (SIPRI trend indicator values)\u0027, \u0027MS_MIL_MPRT_KD\u0027, 1960, 7000000.0)\n(\u0027Austria\u0027, \u0027AUT\u0027, \u0027Birth rate crude (per 1000 people)\u0027, \u0027SP_DYN_CBRT_IN\u0027, 1960, 17.9)\n(\u0027Austria\u0027, \u0027AUT\u0027, \u0027CO2 emissions (kg per 2005 US$ of GDP)\u0027, \u0027EN_ATM_CO2E_KD_GD\u0027, 1960, 0.388626120450145)\n(\u0027Austria\u0027, \u0027AUT\u0027, \u0027CO2 emissions (kt)\u0027, \u0027EN_ATM_CO2E_KT\u0027, 1960, 30821.135)\n(\u0027Austria\u0027, \u0027AUT\u0027, \u0027CO2 emissions (metric tons per capita)\u0027, \u0027EN_ATM_CO2E_PC\u0027, 1960, 4.37331882803344)\n"
      },
      "dateCreated": "Mar 12, 2016 9:18:04 PM",
      "dateStarted": "Mar 21, 2016 3:42:38 PM",
      "dateFinished": "Mar 21, 2016 3:42:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nindicatorsDF.printSchema()\n\nsqlContext.registerDataFrameAsTable(indicatorsDF, \"Indicators\")\n",
      "dateUpdated": "Mar 21, 2016 3:58:19 PM",
      "config": {
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457814105491_409709232",
      "id": "20160312-212145_1125570150",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "root\n |-- CountryName: string (nullable \u003d true)\n |-- CountryCode: string (nullable \u003d true)\n |-- IndicatorName: string (nullable \u003d true)\n |-- IndicatorCode: string (nullable \u003d true)\n |-- Year: integer (nullable \u003d true)\n |-- Value: float (nullable \u003d true)\n\n"
      },
      "dateCreated": "Mar 12, 2016 9:21:45 PM",
      "dateStarted": "Mar 21, 2016 3:42:50 PM",
      "dateFinished": "Mar 21, 2016 3:42:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nimport re\nfrom pyspark.sql.types import *\n\nshortSchema \u003d StructType([ \\\n   StructField(\"CountryCode\",   StringType(),  True), \\\n   StructField(\"IndicatorCode\", StringType(),  True), \\\n   StructField(\"Year\",          IntegerType(), True), \\\n   StructField(\"Value\",         FloatType(),  True)  \\\n])\n\nindicatorsSubset \u003d indicators.map(lambda row: (row[0]))\n\nfor i in indicatorsSubset.take(10):\n    print i\n",
      "dateUpdated": "Mar 21, 2016 3:58:19 PM",
      "config": {
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458572111064_-355638896",
      "id": "20160321-155511_189719602",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 536.0 failed 1 times, most recent failure: Lost task 0.0 in stage 536.0 (TID 1555, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/hadoop/spark-1.3.1-bin-hadoop1/python/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/Users/hadoop/spark-1.3.1-bin-hadoop1/python/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/hadoop/spark-1.3.1-bin-hadoop1/python/pyspark/serializers.py\", line 236, in dump_stream\n    vs \u003d list(itertools.islice(iterator, batch))\n  File \"/Users/hadoop/spark-1.3.1-bin-hadoop1/python/pyspark/rdd.py\", line 1220, in takeUpToNumLeft\n    yield next(iterator)\n  File \"\u003cstring\u003e\", line 17, in \u003clambda\u003e\nValueError: invalid literal for int() with base 10: \u0027Year\u0027\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.\u003cinit\u003e(PythonRDD.scala:176)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\n(\u003cclass \u0027py4j.protocol.Py4JJavaError\u0027\u003e, Py4JJavaError(u\u0027An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\\n\u0027, JavaObject id\u003do6755), \u003ctraceback object at 0x1041ea488\u003e)"
      },
      "dateCreated": "Mar 21, 2016 3:55:11 PM",
      "dateStarted": "Mar 21, 2016 3:57:40 PM",
      "dateFinished": "Mar 21, 2016 3:57:40 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\nselect *\nfrom Indicators\nlimit 10",
      "dateUpdated": "Mar 21, 2016 3:58:19 PM",
      "config": {
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "CountryName",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "CountryCode",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "CountryName",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "CountryCode",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458570114277_-439363082",
      "id": "20160321-152154_333447080",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "CountryName\tCountryCode\tIndicatorName\tIndicatorCode\tYear\tValue\nAustria\tAUT\tAdolescent fertility rate (births per 1000 women ages 15-19)\tSP_ADO_TFRT\t1960\t48.3914\nAustria\tAUT\tAge dependency ratio (% of working-age population)\tSP_POP_DPND\t1960\t52.34817\nAustria\tAUT\tAge dependency ratio old (% of working-age population)\tSP_POP_DPND_OL\t1960\t18.49511\nAustria\tAUT\tAge dependency ratio young (% of working-age population)\tSP_POP_DPND_YG\t1960\t33.85306\nAustria\tAUT\tAlternative and nuclear energy (% of total energy use)\tEG_USE_COMM_CL_ZS\t1960\t8.964336\nAustria\tAUT\tArms imports (SIPRI trend indicator values)\tMS_MIL_MPRT_KD\t1960\t7000000.0\nAustria\tAUT\tBirth rate crude (per 1000 people)\tSP_DYN_CBRT_IN\t1960\t17.9\nAustria\tAUT\tCO2 emissions (kg per 2005 US$ of GDP)\tEN_ATM_CO2E_KD_GD\t1960\t0.38862613\nAustria\tAUT\tCO2 emissions (kt)\tEN_ATM_CO2E_KT\t1960\t30821.135\nAustria\tAUT\tCO2 emissions (metric tons per capita)\tEN_ATM_CO2E_PC\t1960\t4.3733187\n"
      },
      "dateCreated": "Mar 21, 2016 3:21:54 PM",
      "dateStarted": "Mar 21, 2016 3:48:23 PM",
      "dateFinished": "Mar 21, 2016 3:48:23 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Transform Indicators table to columns\n\n- replace the dots in the IndicatorCode column with underscores",
      "dateUpdated": "Mar 21, 2016 3:58:19 PM",
      "config": {
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458567429768_21134855",
      "id": "20160321-143709_2059655775",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003eTransform Indicators table to columns\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ereplace the dots in the IndicatorCode column with underscores\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Mar 21, 2016 2:37:09 PM",
      "dateStarted": "Mar 21, 2016 2:52:55 PM",
      "dateFinished": "Mar 21, 2016 2:52:55 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\ncolumns \u003d indicatorsDF.map(lambda row: row.IndicatorCode.replace(\".\", \"_\")).distinct().collect()\nbc \u003d sc.broadcast(columns)\n\ndef seq(u, v):\n    if u \u003d\u003d None: \n        u \u003d {ind: None for ind in bc.values}\n    u[v.IndicatorCode.replace(\".\", \"_\")] \u003d v.Value\n    return u\n    \ndef comb(u1, u2):\n    u1.update(u2)\n    return u1\n\ndef merge(keys, value):\n    value[\"Country\"] \u003d keys[0]\n    value[\"Year\"] \u003d int(keys[1])\n    return Row(**value)\n",
      "dateUpdated": "Mar 21, 2016 3:58:19 PM",
      "config": {
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458567801620_2014687472",
      "id": "20160321-144321_1651512422",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 432.0 failed 1 times, most recent failure: Lost task 1.0 in stage 432.0 (TID 459, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/hadoop/spark-1.3.1-bin-hadoop1/python/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/Users/hadoop/spark-1.3.1-bin-hadoop1/python/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/hadoop/spark-1.3.1-bin-hadoop1/python/pyspark/serializers.py\", line 236, in dump_stream\n    vs \u003d list(itertools.islice(iterator, batch))\n  File \"\u003cstring\u003e\", line 17, in \u003clambda\u003e\nValueError: invalid literal for int() with base 10: \u0027SP.DYN.AMRT.FE\u0027\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:98)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.next(PythonRDD.scala:94)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:122)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:114)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:114)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:421)\n\tat org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:243)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)\n\tat org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:205)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\n(\u003cclass \u0027py4j.protocol.Py4JJavaError\u0027\u003e, Py4JJavaError(u\u0027An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\\n\u0027, JavaObject id\u003do5873), \u003ctraceback object at 0x104114e18\u003e)"
      },
      "dateCreated": "Mar 21, 2016 2:43:21 PM",
      "dateStarted": "Mar 21, 2016 3:28:01 PM",
      "dateFinished": "Mar 21, 2016 3:28:01 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "dateUpdated": "Mar 21, 2016 3:58:19 PM",
      "config": {
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1458568025639_-94030948",
      "id": "20160321-144705_2127854681",
      "dateCreated": "Mar 21, 2016 2:47:05 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Lab4 Spark QL Regex Version",
  "id": "2BFPXW2Z2",
  "angularObjects": {
    "2BCE6R8T2": [],
    "2BDMU83T5": [],
    "2BEMMQPJG": [],
    "2BDW72X9E": [],
    "2BF3AAW7U": [],
    "2BEJ4D5SJ": [],
    "2BF7MH2JM": [],
    "2BE1TRU6B": [],
    "2BE5CQ2W8": [],
    "2BFUXNJ9R": [],
    "2BCQ6J7ZP": [],
    "2BCWJF462": [],
    "2BFDNS2ZU": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}